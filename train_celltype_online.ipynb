{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: no DISPLAY environment variable.\n",
      "--No graphics will be displayed.\n"
     ]
    }
   ],
   "source": [
    "import os.path as op\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import hnn_core\n",
    "from hnn_core import calcium_model, simulate_dipole, read_params, pick_connection\n",
    "from hnn_core.network_models import add_erp_drives_to_jones_model\n",
    "from hnn_core.network_builder import NetworkBuilder\n",
    "from hnn_core.cell import _get_gaussian_connection\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from neurodsp.spectral import compute_spectrum, trim_spectrum\n",
    "import scipy\n",
    "import utils\n",
    "from utils import (SingleNeuron_Data, Network_Data, CellType_Dataset_Fast,\n",
    "                   linear_scale_forward, log_scale_forward, UniformPrior,\n",
    "                   section_drive_param_function, ConcatTensorDataset)\n",
    "import multiprocessing\n",
    "import dill\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "device = torch.device(\"cuda:0\")\n",
    "num_cores = multiprocessing.cpu_count()\n",
    "torch.backends.cudnn.enabled = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cores = 16\n",
    "\n",
    "\n",
    "# Define simulation function\n",
    "#---------------------------\n",
    "def run_hnn(thetai, sample_idx, prior_dict, transform_dict=None, suffix='subthreshold', rate=20, cell_type='L5_pyramidal',save=False):\n",
    "\n",
    "    data_path = f'/users/ntolley/scratch/bayesian_surrogates/datasets_{suffix}'\n",
    "\n",
    "    theta_dict = {param_name: param_dict['rescale_function'](thetai[param_idx].numpy(), param_dict['bounds']) for \n",
    "                    param_idx, (param_name, param_dict) in enumerate(prior_dict.items())}\n",
    "\n",
    "    params = hnn_core.params_default.get_params_default()\n",
    "    params.update({'N_pyr_x': 3, 'N_pyr_y': 3})\n",
    "    \n",
    "    net = calcium_model(params)\n",
    "    if suffix != 'connected':\n",
    "        net.clear_connectivity()\n",
    "\n",
    "    theta_extra = {'cell_type_lookup': cell_type_lookup, 'valid_conn_list': list(),\n",
    "                   'valid_drive_dict': dict()}\n",
    "    # Connections\n",
    "    for conn_idx in range(len(net.connectivity)):\n",
    "        src_type = cell_type_lookup[net.connectivity[conn_idx]['src_type']]\n",
    "        target_type = cell_type_lookup[net.connectivity[conn_idx]['target_type']]\n",
    "        receptor = net.connectivity[conn_idx]['receptor']\n",
    "        loc = net.connectivity[conn_idx]['loc']\n",
    "        \n",
    "        conn_name = f'{src_type}_{target_type}_{receptor}_{loc}'\n",
    "        theta_extra['valid_conn_list'].append(conn_name)\n",
    "        theta_extra[f'{conn_name}_conn_idx'] = conn_idx\n",
    "\n",
    "    # Drives\n",
    "    for cell_type in net.cell_types.keys():\n",
    "        for sec_name in net.cell_types[cell_type].sections.keys():\n",
    "            for syn_name in net.cell_types[cell_type].sections[sec_name].syns:\n",
    "                drive_name = f'{cell_type}_{sec_name}_{syn_name}'\n",
    "                theta_extra['valid_drive_dict'][drive_name] = {\n",
    "                    'cell_type': cell_type, 'location': sec_name, 'receptor': syn_name}\n",
    "\n",
    "    theta_extra['sample_idx'] =  sample_idx\n",
    "    theta_dict['theta_extra'] = theta_extra\n",
    "\n",
    "    section_drive_param_function(net, theta_dict, rate=rate)\n",
    "    dpl = simulate_dipole(net, dt=0.5, tstop=1000, record_vsec='all', record_isec='all', record_dcell=True)\n",
    "\n",
    "    for cell_type in net.cell_types.keys():\n",
    "        if transform_dict is None:\n",
    "            input_spike_scaler, vsec_scaler, isec_scaler = None, None, None\n",
    "        else:\n",
    "            input_spike_scaler = transform_dict[cell_type]['input_spike_scaler']\n",
    "            vsec_scaler = transform_dict[cell_type]['vsec_scaler']\n",
    "            isec_scaler = transform_dict[cell_type]['isec_scaler']\n",
    "\n",
    "        training_set = utils.CellType_Dataset_Fast(\n",
    "            net, cell_type=cell_type, window_size=500, data_step_size=250,\n",
    "            input_spike_scaler=input_spike_scaler, vsec_scaler=vsec_scaler, isec_scaler=isec_scaler,\n",
    "            soma_filter=False, device='cpu')\n",
    "        \n",
    "        if save == True:\n",
    "            torch.save(training_set, f'{data_path}/training_data/{cell_type}_dataset_online.pt')\n",
    "    return training_set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LSTM/GRU architecture for decoding\n",
    "class model_lstm(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_dim=64, n_layers=5, dropout=0.1, kernel_size=200, device='cuda:0', bidirectional=False):\n",
    "        super(model_lstm, self).__init__()\n",
    "\n",
    "        #multiplier based on bidirectional parameter\n",
    "        if bidirectional:\n",
    "            num_directions = 2\n",
    "        else:\n",
    "            num_directions = 1\n",
    "\n",
    "        # Defining some parameters\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers * num_directions\n",
    "        self.device = device\n",
    "        self.dropout = dropout\n",
    "        self.bidirectional = bidirectional\n",
    "        self.kernel_size = kernel_size\n",
    "\n",
    "        self.tau1_init, self.tau2_init = 10, 20\n",
    "        self.tau1 = nn.Parameter(torch.tensor(1).float().to(self.device))\n",
    "        self.tau2 = nn.Parameter(torch.tensor(1).float().to(self.device))\n",
    "\n",
    "        self.kernel_scale_init, self.kernel_offset_init = 10, -5\n",
    "        self.kernel_scale = nn.Parameter(torch.tensor(1).float().to(self.device))\n",
    "        self.kernel_offset = nn.Parameter(torch.tensor(1).float().to(self.device))\n",
    "\n",
    "        # LSTM Layer\n",
    "        # self.lstm = nn.LSTM(hidden_dim, hidden_dim, n_layers, batch_first=True, dropout=dropout)\n",
    "        self.lstm = nn.LSTM(input_size, hidden_dim, n_layers, batch_first=True, dropout=dropout)   \n",
    "\n",
    "        self.fc_input = nn.Sequential(\n",
    "            nn.Linear(input_size, self.hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(self.hidden_dim, self.hidden_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "        self.fc_output = nn.Sequential(\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim*num_directions, self.hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(self.hidden_dim, self.hidden_dim),\n",
    "            nn.Linear(self.hidden_dim, self.output_size)\n",
    "\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        batch_size = x.size(0)\n",
    "        kernel = self.get_kernel(torch.arange(0, self.kernel_size, 1).to(self.device),\n",
    "                                tau1=self.tau1_init, tau2=self.tau2_init).float().flip(0)\n",
    "        # kernel = (kernel * self.kernel_scale * self.kernel_scale_init) + (self.kernel_offset * self.kernel_offset_init)\n",
    "        kernel_product = kernel.tile(dims=(batch_size, self.input_size, 1)).transpose(1,2)\n",
    "\n",
    "        # out = torch.log10(x+10.0)\n",
    "        out = (kernel_product * x).sum(dim=1).unsqueeze(1)\n",
    "        out = out - self.kernel_offset\n",
    "        # print(out.shape)\n",
    "\n",
    "        # out = self.fc_input(out.contiguous())\n",
    "        out, hidden = self.lstm(out, hidden)\n",
    "        out = out.contiguous()\n",
    "        out = self.fc_output(out)\n",
    "            \n",
    "        return out, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        # This method generates the first hidden state of zeros which we'll use in the forward pass\n",
    "        weight = next(self.parameters()).data.to(self.device)\n",
    "\n",
    "        # LSTM cell initialization\n",
    "        hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(self.device),\n",
    "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(self.device))\n",
    "    \n",
    "        return hidden\n",
    "\n",
    "    def get_kernel(self, t_vec, tau1=10, tau2=20):\n",
    "        G = tau2/(tau2-tau1)*(-torch.exp(-t_vec/tau1) + torch.exp(-t_vec/tau2))\n",
    "        return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joblib will run 1 trial(s) in parallel by distributing trials over 1 jobs.\n",
      "Loading custom mechanism files from /oscar/home/ntolley/Jones_Lab/bayesian_surrogates/externals/hnn-core/hnn_core/mod/x86_64/libnrnmech.so\n",
      "Building the NEURON model\n",
      "[Done]\n",
      "Trial 1: 0.5 ms...\n",
      "Trial 1: 10.0 ms...\n",
      "Trial 1: 20.0 ms...\n",
      "Trial 1: 30.0 ms...\n",
      "Trial 1: 40.0 ms...\n",
      "Trial 1: 50.0 ms...\n",
      "Trial 1: 60.0 ms...\n",
      "Trial 1: 70.0 ms...\n",
      "Trial 1: 80.0 ms...\n",
      "Trial 1: 90.0 ms...\n",
      "Trial 1: 100.0 ms...\n",
      "Trial 1: 110.0 ms...\n",
      "Trial 1: 120.0 ms...\n",
      "Trial 1: 130.0 ms...\n",
      "Trial 1: 140.0 ms...\n",
      "Trial 1: 150.0 ms...\n",
      "Trial 1: 160.0 ms...\n",
      "Trial 1: 170.0 ms...\n",
      "Trial 1: 180.0 ms...\n",
      "Trial 1: 190.0 ms...\n",
      "Trial 1: 200.0 ms...\n",
      "Trial 1: 210.0 ms...\n",
      "Trial 1: 220.0 ms...\n",
      "Trial 1: 230.0 ms...\n",
      "Trial 1: 240.0 ms...\n",
      "Trial 1: 250.0 ms...\n",
      "Trial 1: 260.0 ms...\n",
      "Trial 1: 270.0 ms...\n",
      "Trial 1: 280.0 ms...\n",
      "Trial 1: 290.0 ms...\n",
      "Trial 1: 300.0 ms...\n",
      "Trial 1: 310.0 ms...\n",
      "Trial 1: 320.0 ms...\n",
      "Trial 1: 330.0 ms...\n",
      "Trial 1: 340.0 ms...\n",
      "Trial 1: 350.0 ms...\n",
      "Trial 1: 360.0 ms...\n",
      "Trial 1: 370.0 ms...\n",
      "Trial 1: 380.0 ms...\n",
      "Trial 1: 390.0 ms...\n",
      "Trial 1: 400.0 ms...\n",
      "Trial 1: 410.0 ms...\n",
      "Trial 1: 420.0 ms...\n",
      "Trial 1: 430.0 ms...\n",
      "Trial 1: 440.0 ms...\n",
      "Trial 1: 450.0 ms...\n",
      "Trial 1: 460.0 ms...\n",
      "Trial 1: 470.0 ms...\n",
      "Trial 1: 480.0 ms...\n",
      "Trial 1: 490.0 ms...\n",
      "Trial 1: 500.0 ms...\n",
      "Trial 1: 510.0 ms...\n",
      "Trial 1: 520.0 ms...\n",
      "Trial 1: 530.0 ms...\n",
      "Trial 1: 540.0 ms...\n",
      "Trial 1: 550.0 ms...\n",
      "Trial 1: 560.0 ms...\n",
      "Trial 1: 570.0 ms...\n",
      "Trial 1: 580.0 ms...\n",
      "Trial 1: 590.0 ms...\n",
      "Trial 1: 600.0 ms...\n",
      "Trial 1: 610.0 ms...\n",
      "Trial 1: 620.0 ms...\n",
      "Trial 1: 630.0 ms...\n",
      "Trial 1: 640.0 ms...\n",
      "Trial 1: 650.0 ms...\n",
      "Trial 1: 660.0 ms...\n",
      "Trial 1: 670.0 ms...\n",
      "Trial 1: 680.0 ms...\n",
      "Trial 1: 690.0 ms...\n",
      "Trial 1: 700.0 ms...\n",
      "Trial 1: 710.0 ms...\n",
      "Trial 1: 720.0 ms...\n",
      "Trial 1: 730.0 ms...\n",
      "Trial 1: 740.0 ms...\n",
      "Trial 1: 750.0 ms...\n",
      "Trial 1: 760.0 ms...\n",
      "Trial 1: 770.0 ms...\n",
      "Trial 1: 780.0 ms...\n",
      "Trial 1: 790.0 ms...\n",
      "Trial 1: 800.0 ms...\n",
      "Trial 1: 810.0 ms...\n",
      "Trial 1: 820.0 ms...\n",
      "Trial 1: 830.0 ms...\n",
      "Trial 1: 840.0 ms...\n",
      "Trial 1: 850.0 ms...\n",
      "Trial 1: 860.0 ms...\n",
      "Trial 1: 870.0 ms...\n",
      "Trial 1: 880.0 ms...\n",
      "Trial 1: 890.0 ms...\n",
      "Trial 1: 900.0 ms...\n",
      "Trial 1: 910.0 ms...\n",
      "Trial 1: 920.0 ms...\n",
      "Trial 1: 930.0 ms...\n",
      "Trial 1: 940.0 ms...\n",
      "Trial 1: 950.0 ms...\n",
      "Trial 1: 960.0 ms...\n",
      "Trial 1: 970.0 ms...\n",
      "Trial 1: 980.0 ms...\n",
      "Trial 1: 990.0 ms...\n"
     ]
    }
   ],
   "source": [
    "cell_type = 'L5_pyramidal'\n",
    "\n",
    "suffix = 'subthreshold'\n",
    "rate = 20.0\n",
    "net = calcium_model()\n",
    "\n",
    "cell_type_lookup = {\n",
    "    'L2_pyramidal': 'L2e', 'L2_basket': 'L2i',\n",
    "    'L5_pyramidal': 'L5e', 'L5_basket': 'L5i'}\n",
    "\n",
    "\n",
    "prior_dict = dict()\n",
    "prior_dict['lamtha'] = {'bounds': (0, 10), 'rescale_function': linear_scale_forward}\n",
    "\n",
    "# Connections\n",
    "for conn_idx in range(len(net.connectivity)):\n",
    "    src_type = cell_type_lookup[net.connectivity[conn_idx]['src_type']]\n",
    "    target_type = cell_type_lookup[net.connectivity[conn_idx]['target_type']]\n",
    "    receptor = net.connectivity[conn_idx]['receptor']\n",
    "    loc = net.connectivity[conn_idx]['loc']\n",
    "    \n",
    "    conn_name = f'{src_type}_{target_type}_{receptor}_{loc}'\n",
    "    \n",
    "    prior_dict[f'{conn_name}_gbar'] = {'bounds': (-4, 0), 'rescale_function': log_scale_forward}\n",
    "    prior_dict[f'{conn_name}_prob'] = {'bounds': (0, 1), 'rescale_function': linear_scale_forward}\n",
    "\n",
    "# Drives\n",
    "for cell_type in net.cell_types.keys():\n",
    "    for sec_name in net.cell_types[cell_type].sections.keys():\n",
    "        for syn_name in net.cell_types[cell_type].sections[sec_name].syns:\n",
    "            drive_name = f'{cell_type}_{sec_name}_{syn_name}'\n",
    "            prior_dict[f'{drive_name}_gbar'] = {'bounds': (-4, 0), 'rescale_function': log_scale_forward}\n",
    "            prior_dict[f'{drive_name}_prob'] = {'bounds': (0, 1), 'rescale_function': linear_scale_forward}\n",
    "\n",
    "\n",
    "prior = UniformPrior(parameters=list(prior_dict.keys()))\n",
    "theta_samples = prior.sample((2,))\n",
    "\n",
    "# First sample used to fit transformers\n",
    "theta_samples[0,:] = torch.from_numpy(np.repeat(0.7, theta_samples.shape[1]))\n",
    "dataset = run_hnn(theta_samples[0, :], 0, prior_dict, transform_dict=None, suffix=suffix, rate=rate, save=True)\n",
    "\n",
    "transform_dict = {}\n",
    "for cell_type in net.cell_types.keys():\n",
    "    dataset = torch.load(f'/users/ntolley/scratch/bayesian_surrogates/datasets_subthreshold/training_data/{cell_type}_dataset_online.pt')\n",
    "    transform_dict[cell_type] = {'input_spike_scaler': dataset.input_spike_scaler,\n",
    "                                 'vsec_scaler': dataset.vsec_scaler,\n",
    "                                 'isec_scaler': dataset.isec_scaler}\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim = 64\n",
    "n_layers = 8\n",
    "\n",
    "_, input_size = dataset[0][0].detach().cpu().numpy().shape\n",
    "_, output_size = dataset[0][1].detach().cpu().numpy().shape\n",
    "\n",
    "model = utils.model_celltype_lstm(input_size=input_size, output_size=output_size,\n",
    "                                  hidden_dim=hidden_dim, n_layers=n_layers, device=device).to(device)\n",
    "\n",
    "lr = 0.01\n",
    "weight_decay = 1e-3\n",
    "max_epochs = 1000\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: no DISPLAY environment variable.\n",
      "--No graphics will be displayed.\n",
      "Warning: no DISPLAY environment variable.\n",
      "--No graphics will be displayed.\n",
      "Warning: no DISPLAY environment variable.\n",
      "--No graphics will be displayed.\n",
      "Warning: no DISPLAY environment variable.\n",
      "--No graphics will be displayed.\n",
      "Warning: no DISPLAY environment variable.\n",
      "--No graphics will be displayed.\n",
      "Warning: no DISPLAY environment variable.\n",
      "--No graphics will be displayed.\n",
      "Warning: no DISPLAY environment variable.\n",
      "--No graphics will be displayed.\n",
      "Warning: no DISPLAY environment variable.\n",
      "--No graphics will be displayed.\n",
      "Warning: no DISPLAY environment variable.\n",
      "--No graphics will be displayed.\n",
      "Warning: no DISPLAY environment variable.\n",
      "--No graphics will be displayed.\n",
      "Warning: no DISPLAY environment variable.\n",
      "--No graphics will be displayed.\n",
      "Warning: no DISPLAY environment variable.\n",
      "--No graphics will be displayed.\n",
      "Warning: no DISPLAY environment variable.\n",
      "--No graphics will be displayed.\n",
      "Warning: no DISPLAY environment variable.\n",
      "--No graphics will be displayed.\n",
      "Warning: no DISPLAY environment variable.\n",
      "--No graphics will be displayed.\n",
      "Warning: no DISPLAY environment variable.\n",
      "--No graphics will be displayed.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "train_loss_array = list()\n",
    "for batch_idx in range(max_epochs):\n",
    "    theta_samples = prior.sample((num_cores,))\n",
    "    dataset_list = Parallel(n_jobs=num_cores)(delayed(run_hnn)(\n",
    "        thetai, sample_idx+1, prior_dict, transform_dict, suffix, rate, save=False) for\n",
    "        (sample_idx, thetai) in enumerate(theta_samples))\n",
    "\n",
    "    # training_set = ConcatTensorDataset(dataset_list)\n",
    "\n",
    "    # #___Train model___\n",
    "    # model.train()\n",
    "    # train_batch_loss = list()\n",
    "    # validation_batch_loss = list()\n",
    "    # optimizer.zero_grad() # Clears existing gradients from previous epoch\n",
    "    # batch_x = training_set[:][0].float().to(device)\n",
    "    # batch_y = training_set[:][1].float().to(device)\n",
    "\n",
    "    # output_sequence = list()\n",
    "    # h0 = torch.zeros(model.n_layers, batch_x.size(0), model.hidden_dim).to(device)\n",
    "    # c0 = torch.zeros(model.n_layers, batch_x.size(0), model.hidden_dim).to(device)\n",
    "\n",
    "    # for t in range(model.kernel_size, batch_x.size(1)-1):\n",
    "    #     output, h0, c0 = model(batch_x[:,(t-model.kernel_size):t, :], h0, c0)\n",
    "    #     output_sequence.append(output)\n",
    "\n",
    "    # output_sequence = torch.cat(output_sequence, dim=1)\n",
    "    # train_loss = criterion(output_sequence, batch_y[:, model.kernel_size+1:,:])\n",
    "\n",
    "    # train_loss.backward() # Does backpropagation and calculates gradients\n",
    "    # optimizer.step() # Updates the weights accordingly\n",
    "    \n",
    "    # train_loss_array.append(train_loss.detach().cpu().numpy())\n",
    "\n",
    "    # print(f'Loss: {train_loss}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/users/ntolley/Jones_Lab/bayesian_surrogates/train_celltype_online.ipynb Cell 8\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell://ood.ccv.brown.edu/users/ntolley/Jones_Lab/bayesian_surrogates/train_celltype_online.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m dataset_type \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39msubthreshold\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://ood.ccv.brown.edu/users/ntolley/Jones_Lab/bayesian_surrogates/train_celltype_online.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m cell_type \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mL5_pyramidal\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ood.ccv.brown.edu/users/ntolley/Jones_Lab/bayesian_surrogates/train_celltype_online.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m torch\u001b[39m.\u001b[39msave(model\u001b[39m.\u001b[39mstate_dict(), \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mdataset_type\u001b[39m}\u001b[39;00m\u001b[39m_models/\u001b[39m\u001b[39m{\u001b[39;00mcell_type\u001b[39m}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m{\u001b[39;00mdataset_type\u001b[39m}\u001b[39;00m\u001b[39m_model.pt\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ood.ccv.brown.edu/users/ntolley/Jones_Lab/bayesian_surrogates/train_celltype_online.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# with open(f'{dataset_type}_models/{cell_type}_{dataset_type}_loss_dict.pkl', 'wb') as f:\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ood.ccv.brown.edu/users/ntolley/Jones_Lab/bayesian_surrogates/train_celltype_online.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m#     dill.dump(loss_dict, f)\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "dataset_type = 'subthreshold'\n",
    "cell_type = 'L5_pyramidal'\n",
    "\n",
    "torch.save(model.state_dict(), f'{dataset_type}_models/{cell_type}_{dataset_type}_model.pt')\n",
    "# with open(f'{dataset_type}_models/{cell_type}_{dataset_type}_loss_dict.pkl', 'wb') as f:\n",
    "#     dill.dump(loss_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bayes_srgt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
